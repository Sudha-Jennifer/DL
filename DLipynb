{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOyHtfPFG2AnlHd/pCqu1Sk"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdKU-qyZw9a7",
        "outputId": "1ae74d19-0040-44b9-ec71-723a172dd501"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: ./data/train_32x32.mat\n",
            "Using downloaded and verified file: ./data/test_32x32.mat\n",
            "Training with [32, 64, 128], relu, xavier, lr=0.001, optimizer=sgd, batch_size=16, weight_decay=0, epochs=5\n",
            "Using downloaded and verified file: ./data/train_32x32.mat\n",
            "Using downloaded and verified file: ./data/test_32x32.mat\n",
            "Epoch 1/5, Loss: 2.2322, Val Accuracy: 0.1969\n",
            "Epoch 2/5, Loss: 2.0497, Val Accuracy: 0.3563\n",
            "Epoch 3/5, Loss: 1.6385, Val Accuracy: 0.5267\n",
            "Epoch 4/5, Loss: 1.3360, Val Accuracy: 0.6111\n",
            "Epoch 5/5, Loss: 1.1716, Val Accuracy: 0.6528\n",
            "Training with [32, 64, 128], relu, xavier, lr=0.001, optimizer=sgd, batch_size=16, weight_decay=0, epochs=10\n",
            "Using downloaded and verified file: ./data/train_32x32.mat\n",
            "Using downloaded and verified file: ./data/test_32x32.mat\n",
            "Epoch 1/10, Loss: 2.2060, Val Accuracy: 0.2311\n",
            "Epoch 2/10, Loss: 1.9403, Val Accuracy: 0.4431\n",
            "Epoch 3/10, Loss: 1.5011, Val Accuracy: 0.5891\n",
            "Epoch 4/10, Loss: 1.2581, Val Accuracy: 0.6336\n",
            "Epoch 5/10, Loss: 1.1332, Val Accuracy: 0.6699\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "def load_svhn(batch_size=32):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4377, 0.4438, 0.4728), (0.198, 0.201, 0.197))\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.SVHN(root='./data', split='train', transform=transform, download=True)\n",
        "    test_dataset = datasets.SVHN(root='./data', split='test', transform=transform, download=True)\n",
        "\n",
        "    val_size = int(0.1 * len(train_dataset))\n",
        "    train_size = len(train_dataset) - val_size\n",
        "    train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "class FeedforwardNN(nn.Module):\n",
        "    def __init__(self, input_size=3072, hidden_layers=[32, 64, 128], output_size=10, activation='relu', weight_init='xavier'):\n",
        "        super(FeedforwardNN, self).__init__()\n",
        "        layers = []\n",
        "        prev_size = input_size\n",
        "        for h in hidden_layers:\n",
        "            layer = nn.Linear(prev_size, h)\n",
        "            if weight_init == 'xavier':\n",
        "                nn.init.xavier_uniform_(layer.weight)\n",
        "            elif weight_init == 'random':\n",
        "                nn.init.uniform_(layer.weight)\n",
        "            layers.append(layer)\n",
        "            layers.append(nn.ReLU() if activation == 'relu' else nn.Sigmoid())\n",
        "            prev_size = h\n",
        "\n",
        "        layers.append(nn.Linear(prev_size, output_size))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # Flatten images\n",
        "        return self.model(x)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, optimizer, criterion, epochs=10):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        val_acc = evaluate_model(model, val_loader)\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}, Val Accuracy: {val_acc:.4f}')\n",
        "\n",
        "def evaluate_model(model, data_loader):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return accuracy_score(all_labels, all_preds)\n",
        "\n",
        "train_loader, val_loader, test_loader = load_svhn(batch_size=32)\n",
        "\n",
        "# Hyperparameter configuration\n",
        "hidden_layers_options = [[32, 64, 128], [32, 64, 128, 128], [32, 64, 128, 128, 128]]\n",
        "activation_options = ['relu', 'sigmoid']\n",
        "weight_init_options = ['xavier', 'random']\n",
        "learning_rate_options = [1e-3, 1e-4]\n",
        "optimizer_options = ['sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam']\n",
        "batch_size_options = [16, 32, 64]\n",
        "weight_decay_options = [0, 0.0005, 0.5]\n",
        "epochs_options = [5, 10]\n",
        "\n",
        "best_acc = 0\n",
        "best_model = None\n",
        "best_optimizer = None\n",
        "\n",
        "for hidden_layers in hidden_layers_options:\n",
        "    for activation in activation_options:\n",
        "        for weight_init in weight_init_options:\n",
        "            for learning_rate in learning_rate_options:\n",
        "                for optimizer_choice in optimizer_options:\n",
        "                    for batch_size in batch_size_options:\n",
        "                        for weight_decay in weight_decay_options:\n",
        "                            for epochs in epochs_options:\n",
        "                                print(f'Training with {hidden_layers}, {activation}, {weight_init}, lr={learning_rate}, optimizer={optimizer_choice}, batch_size={batch_size}, weight_decay={weight_decay}, epochs={epochs}')\n",
        "                                train_loader, val_loader, test_loader = load_svhn(batch_size=batch_size)\n",
        "                                model = FeedforwardNN(input_size=32*32*3, hidden_layers=hidden_layers, activation=activation, weight_init=weight_init)\n",
        "                                criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "                                optimizer_dict = {\n",
        "                                    'sgd': optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay),\n",
        "                                    'momentum': optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay),\n",
        "                                    'nesterov': optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True, weight_decay=weight_decay),\n",
        "                                    'rmsprop': optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=weight_decay),\n",
        "                                    'adam': optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay),\n",
        "                                    'nadam': optim.NAdam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "                                }\n",
        "\n",
        "                                optimizer = optimizer_dict.get(optimizer_choice, optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay))\n",
        "\n",
        "                                train_model(model, train_loader, val_loader, optimizer, criterion, epochs=epochs)\n",
        "                                val_acc = evaluate_model(model, val_loader)\n",
        "\n",
        "                                if val_acc > best_acc:\n",
        "                                    best_acc = val_acc\n",
        "                                    best_model = model\n",
        "                                    best_optimizer = optimizer_choice\n",
        "\n",
        "print(f'Best Validation Accuracy: {best_acc:.4f} using optimizer {best_optimizer}')\n",
        "test_acc = evaluate_model(best_model, test_loader)\n",
        "print(f'Test Accuracy: {test_acc:.4f}')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A3e7H49A2qMq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}